{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saito/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape the images\n",
    "# store_train = [row.reshape((32, 32, 3)) for row in X_train]\n",
    "# store_test = [row.reshape((32, 32, 3)) for row in X_test]\n",
    "# X_train = store_train\n",
    "# X_test = store_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.swapaxes(X_train, 1, 2)\n",
    "X_train = np.swapaxes(X_train, 2, 3)\n",
    "\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "X_test = np.swapaxes(X_test, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper functions and tools\n",
    "\n",
    "# Batch iterator\n",
    "class DataIterator(object):\n",
    "    def __init__(self, *data, **params):\n",
    "        self.data = data\n",
    "        self.batchsize = params['batchsize']\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.first = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        outs = []\n",
    "        for val in self.data:\n",
    "            outs.append(val[key])\n",
    "        return outs\n",
    "\n",
    "\n",
    "class SequentialIterator(DataIterator):\n",
    "    '''\n",
    "    batchsize = 3\n",
    "    [0, 1, 2], [3, 4, 5], [6, 7, 8]\n",
    "    '''\n",
    "    def next(self):\n",
    "        if self.first >= len(self):\n",
    "            raise StopIteration()\n",
    "        outs = []\n",
    "        for val in self.data:\n",
    "            outs.append(val[self.first:self.first+self.batchsize])\n",
    "        self.first += self.batchsize\n",
    "        return outs\n",
    "\n",
    "# One-hot vectorizer\n",
    "\n",
    "def one_hot(vec, num_classes):\n",
    "    to_return = np.zeros((vec.shape[0], num_classes))\n",
    "    for idx, row in enumerate(vec):\n",
    "        to_return[idx, row] = 1.0\n",
    "    return to_return\n",
    "\n",
    "y_train_onehot = one_hot(y_train, 10)\n",
    "y_test_onehot = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some global variables\n",
    "\n",
    "x_dim = X_train[0].shape[1]\n",
    "num_labels = 10\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "num_channels = 3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x_ph = tf.placeholder(\"float32\", [None, x_dim, x_dim, num_channels])\n",
    "y_ph = tf.placeholder(\"float32\", shape=(None, num_labels))\n",
    "\n",
    "# Weights and biases\n",
    "# Conv layers\n",
    "conv1_w = tf.Variable(tf.truncated_normal([5, 5, 3, 32],stddev=0.1,\n",
    "                                         dtype=\"float32\"))\n",
    "conv1_b = tf.Variable(tf.zeros([32], dtype=\"float32\"))\n",
    "\n",
    "conv2_w = tf.Variable(tf.truncated_normal([5, 5, 32, 64],\n",
    "                            stddev=0.1, dtype=\"float32\"))\n",
    "conv2_b = tf.Variable(tf.zeros([64], dtype=\"float32\"))\n",
    "\n",
    "# Fully-connected layers\n",
    "fc1_w = tf.Variable(tf.truncated_normal([\n",
    "            x_dim // 4 * x_dim // 4 * 64, 512\n",
    "        ], stddev=0.1, dtype=\"float32\"))\n",
    "fc1_b = tf.Variable(tf.constant(0.1, shape=[512], dtype=\"float32\"))\n",
    "\n",
    "fc2_w = tf.Variable(tf.truncated_normal([\n",
    "            512, num_labels\n",
    "        ], stddev=0.1, dtype=\"float32\"))\n",
    "fc2_b = tf.Variable(tf.constant(0.1, shape=[num_labels],\n",
    "                               dtype=\"float32\"))\n",
    "\n",
    "# Main model\n",
    "conv1 = tf.nn.conv2d(x_ph, conv1_w, strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\")\n",
    "relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b))\n",
    "pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1],\n",
    "                      strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "conv2 = tf.nn.conv2d(pool1, conv2_w, strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\")\n",
    "relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1],\n",
    "                      strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# pool_shape = pool2.get_shape().as_list()\n",
    "pool_shape = tf.shape(pool2)\n",
    "reshape = tf.reshape(\n",
    "    pool2, \n",
    "    [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "\n",
    "# Fully-connected layer\n",
    "hidden1 = tf.nn.relu(tf.matmul(reshape, fc1_w) + fc1_b)\n",
    "hidden2_logits = tf.matmul(hidden1, fc2_w) + fc2_b\n",
    "\n",
    "# Output\n",
    "output = tf.nn.softmax(hidden2_logits)\n",
    "\n",
    "# Cross Entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ph * tf.log(tf.clip_by_value(output, \n",
    "        1e-10, 1.0)), reduction_indices=[1]))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "data = SequentialIterator(X_train, y_train_onehot, batchsize=batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "prev_error = 1000000\n",
    "errors = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    training_error = 0\n",
    "    for x_batch, y_batch in data:\n",
    "        feed_dict = {x_ph: x_batch, y_ph: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        output = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "        print output\n",
    "        training_error += output\n",
    "    \n",
    "    print \"Epoch %i\\t, Training Error %f\" % (i, training_error)\n",
    "    if training_error > prev_error:\n",
    "        break\n",
    "    else:\n",
    "        prev_error = training_error\n",
    "    errors.append(training_error)\n",
    "\n",
    "t2 = time.time()\n",
    "print \"Time taken to train: \", t2-t1, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR (Using Caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import caffe\n",
    "\n",
    "solver = caffe.SGDSolver(\"cifar10_quick_solver.prototxt\")\n",
    "solver.solve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
